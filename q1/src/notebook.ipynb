{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# EdTech Math Tutor - Implementation and Evaluation\n",
    "\n",
    "This notebook implements and evaluates a math tutoring system using the Phi-2 language model via Ollama.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Dependencies\n",
    "2. Model Initialization\n",
    "3. Prompt Testing\n",
    "4. Evaluation Framework\n",
    "5. Results Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Union\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Constants\n",
    "OLLAMA_API = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"phi\"\n",
    "\n",
    "# Load prompts\n",
    "def load_prompt(filename: str) -> str:\n",
    "    with open(f\"../prompts/{filename}\", \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "# Load test cases\n",
    "def load_test_cases() -> List[Dict]:\n",
    "    with open(\"../evaluation/input_queries.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data[\"test_cases\"]\n",
    "\n",
    "# Initialize prompt templates\n",
    "zero_shot_template = load_prompt(\"zero_shot.txt\")\n",
    "few_shot_template = load_prompt(\"few_shot.txt\")\n",
    "cot_template = load_prompt(\"cot_prompt.txt\")\n",
    "meta_template = load_prompt(\"meta_prompt.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(prompt: str, max_tokens: int = 1000) -> str:\n",
    "    \"\"\"Query the Ollama model with a prompt.\"\"\"\n",
    "    data = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying model: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def evaluate_response(response: str, expected: Union[float, List[float]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate the model's response against expected answer.\n",
    "    Returns metrics including accuracy and reasoning quality.\n",
    "    \"\"\"\n",
    "    # Extract numerical answer from response\n",
    "    # This is a simple implementation - you might want to make it more robust\n",
    "    try:\n",
    "        if isinstance(expected, list):\n",
    "            # For problems with multiple answers (like quadratic equations)\n",
    "            actual = [float(x) for x in response.split() if x.replace('.','').isdigit()]\n",
    "            accuracy = all(any(abs(a - e) < 0.1 for e in expected) for a in actual)\n",
    "        else:\n",
    "            # For problems with single numerical answer\n",
    "            actual = float([x for x in response.split() if x.replace('.','').isdigit()][0])\n",
    "            accuracy = abs(actual - expected) < 0.1\n",
    "            \n",
    "        # Evaluate reasoning quality (simple heuristics)\n",
    "        reasoning_score = min(5, len(response.split('\\n'))) / 5  # More steps = better reasoning\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": 1 if accuracy else 0,\n",
    "            \"reasoning_quality\": reasoning_score,\n",
    "            \"response_length\": len(response)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating response: {e}\")\n",
    "        return {\n",
    "            \"accuracy\": 0,\n",
    "            \"reasoning_quality\": 0,\n",
    "            \"response_length\": len(response)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating problem 1...\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation for each prompt type\n",
    "def run_evaluation():\n",
    "    test_cases = load_test_cases()\n",
    "    results = []\n",
    "    \n",
    "    prompt_types = {\n",
    "        \"zero_shot\": zero_shot_template,\n",
    "        \"few_shot\": few_shot_template,\n",
    "        \"chain_of_thought\": cot_template,\n",
    "        \"meta_prompt\": meta_template\n",
    "    }\n",
    "    \n",
    "    for case in test_cases:\n",
    "        print(f\"Evaluating problem {case['id']}...\")\n",
    "        \n",
    "        for prompt_type, template in prompt_types.items():\n",
    "            # Prepare prompt\n",
    "            prompt = template.replace(\"[PROBLEM]\", case[\"problem\"])\n",
    "            \n",
    "            # Query model\n",
    "            response = query_model(prompt)\n",
    "            \n",
    "            # Evaluate response\n",
    "            metrics = evaluate_response(response, case[\"expected_answer\"])\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"problem_id\": case[\"id\"],\n",
    "                \"category\": case[\"category\"],\n",
    "                \"difficulty\": case[\"difficulty\"],\n",
    "                \"prompt_type\": prompt_type,\n",
    "                **metrics\n",
    "            })\n",
    "            \n",
    "            # Add delay to avoid overwhelming the API\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run evaluation\n",
    "results_df = run_evaluation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "def plot_results(df: pd.DataFrame):\n",
    "    # Set style\n",
    "    plt.style.use('seaborn')\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Accuracy by prompt type\n",
    "    sns.barplot(\n",
    "        data=df,\n",
    "        x='prompt_type',\n",
    "        y='accuracy',\n",
    "        ax=axes[0,0]\n",
    "    )\n",
    "    axes[0,0].set_title('Accuracy by Prompt Type')\n",
    "    axes[0,0].set_xticklabels(axes[0,0].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # 2. Reasoning quality by prompt type\n",
    "    sns.barplot(\n",
    "        data=df,\n",
    "        x='prompt_type',\n",
    "        y='reasoning_quality',\n",
    "        ax=axes[0,1]\n",
    "    )\n",
    "    axes[0,1].set_title('Reasoning Quality by Prompt Type')\n",
    "    axes[0,1].set_xticklabels(axes[0,1].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # 3. Accuracy by difficulty\n",
    "    sns.barplot(\n",
    "        data=df,\n",
    "        x='difficulty',\n",
    "        y='accuracy',\n",
    "        hue='prompt_type',\n",
    "        ax=axes[1,0]\n",
    "    )\n",
    "    axes[1,0].set_title('Accuracy by Difficulty and Prompt Type')\n",
    "    axes[1,0].set_xticklabels(axes[1,0].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # 4. Accuracy by category\n",
    "    sns.barplot(\n",
    "        data=df,\n",
    "        x='category',\n",
    "        y='accuracy',\n",
    "        hue='prompt_type',\n",
    "        ax=axes[1,1]\n",
    "    )\n",
    "    axes[1,1].set_title('Accuracy by Category and Prompt Type')\n",
    "    axes[1,1].set_xticklabels(axes[1,1].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate plots\n",
    "plot_results(results_df)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"\\nAccuracy by Prompt Type:\")\n",
    "print(results_df.groupby('prompt_type')['accuracy'].mean())\n",
    "\n",
    "print(\"\\nReasoning Quality by Prompt Type:\")\n",
    "print(results_df.groupby('prompt_type')['reasoning_quality'].mean())\n",
    "\n",
    "print(\"\\nAccuracy by Difficulty:\")\n",
    "print(results_df.groupby(['difficulty', 'prompt_type'])['accuracy'].mean().unstack())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
